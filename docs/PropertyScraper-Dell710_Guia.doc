{\rtf1\ansi\deff0{\fonttbl{\f0 Arial;}{\f1 Courier New;}}\n\fs36 \b Guía Completa de Operación – PropertyScraper Dell710\b0\par\par\fs24 
2025-08-29 16:11\par\par
\par\fs28 \b Guía Completa de Operación – PropertyScraper Dell710\b0\fs22 \par
\par
\par\fs24 \b Resumen ejecutivo\b0\fs22 \par
\par
PropertyScraper Dell710 es un sistema profesional de scraping inmobiliario, optimizado para correr sin interfaz gráfica en un servidor Dell PowerEdge T710 (Ubuntu Server 24) y controlarse desde Windows 11 por SSH. Incluye:\par
\par \bullet Scrapers headless con técnicas anti-detección.\par
\par \bullet Orquestación concurrente con control de recursos (CPU/Memoria).\par
\par \bullet Programación bi-mensual automática (1–2 y 15–16, 02:00).\par
\par \bullet Despliegue y ejecución remota por SSH.\par
\par \bullet Monitoreo de performance y logs centralizados.\par
\par \bullet Respaldo automático a Google Drive con rclone sin bloquear los scrapers.\par
\par
Objetivo: extraer, de forma robusta y repetible, listados de propiedades para venta/renta desde múltiples portales, almacenarlos en CSV con metadatos y mantener un historial ordenado por sitio/operación/mes/ejecución.\par
\par
\par\fs24 \b Arquitectura y componentes\b0\fs22 \par
\par
\par \bullet scrapers/: scrapers por portal con modo 100% headless.\par
\par \bullet inmuebles24_professional.py (probado)\par
\par \bullet casas_y_terrenos_scraper.py (implementado)\par
\par \bullet mitula_scraper.py (implementado)\par
\par \bullet [pendientes planificados]: lamudi, lamudi_unico, propiedades, segundamano, trovit, inmuebles24_unico\par
\par \bullet orchestrator/: coordinación y concurrencia.\par
\par \bullet concurrent_manager.py: gestiona colas, hilos y límites por recursos (DellT710ResourceMonitor).\par
\par \bullet bimonthly_scheduler.py: programador bi-mensual que invoca scrapers (vía SSH executor si aplica).\par
\par \bullet ssh_deployment/remote_executor.py: despliegue, sync e invocación remota en el servidor por Paramiko.\par
\par \bullet monitoring/performance_monitor.py: métricas y alertas de CPU/Memoria/Disco/Red.\par
\par \bullet utils/\par
\par \bullet create_data_structure.py: genera árbol de data por sitio/operación/mes/ejecución.\par
\par \bullet gdrive_backup_manager.py: respaldo a Google Drive por rclone; soporta modo automático en background.\par
\par \bullet config/\par
\par \bullet dell_t710_config.yaml: umbrales de recursos, horarios, rutas remotas.\par
\par \bullet ssh_config.json: conexión y paths remotos (sin comentarios JSON).\par
\par \bullet data/: salida en CSV + metadata por ejecución.\par
\par \bullet logs/: logs por módulo, historial de ejecuciones y checkpoints.\par
\par \bullet docs/: documentación (esta guía y docs previos).\par
\par
Stack principal: Python 3.12+, SeleniumBase/Selenium, Paramiko, psutil, schedule, pandas, rclone (CLI), logging estructurado.\par
\par
\par\fs24 \b Flujo de trabajo end-to-end\b0\fs22 \par
\par
1) Programación (bimonthly_scheduler.py) verifica si corresponde ejecución (1–2 o 15–16) y hora (02:00). \par
2) Orquestación (concurrent_manager.py) calcula scrapers concurrentes óptimos según CPU/Memoria (máx 4) y lanza hilos.\par
3) Cada scraper opera en Chrome headless, navega, extrae y guarda CSV+metadata con checkpoints periódicos.\par
4) Al terminar un scraper, se dispara respaldo a Drive con gdrive_backup_manager (por lotes, no bloqueante si se usa modo automático).\par
5) Monitoreo registra métricas y alerta si se superan umbrales.\par
6) Logs centralizados permiten auditoría y diagnóstico.\par
\par
\par\fs24 \b Organización de datos\b0\fs22 \par
\par
Ruta: data/[sitio]/[operacion]/[Mes Año]/[1er_script_del_mes|2do_script_del_mes]\par
\par \bullet CSV resultado principal (uno o varios por ejecución).\par
\par \bullet metadata_*.json con timing, parámetros, totales extraídos.\par
\par \bullet Logs asociados en logs/ con timestamp.\par
\par
La utilidad utils/create_data_structure.py pre-crea todas las carpetas futuras (meses/ejecuciones), de modo que los scrapers sólo escriben.\par
\par
\par\fs24 \b Orquestación y concurrencia\b0\fs22 \par
\par
\par \bullet DellT710ResourceMonitor:\par
\par \bullet CPU máx: 80%; Memoria máx: 80% (24 GB → 19.2 GB).\par
\par \bullet Estimación por scraper: ~15% CPU, ~3 GB RAM.\par
\par \bullet Concurrency óptima: min(80/15, 19.2/3, 4) → máx 4 scrapers.\par
\par \bullet ConcurrentScraperManager:\par
\par \bullet Cola de tareas: \{site, operation, headless, max_pages, priority\}.\par
\par \bullet Lanza hilos con seguridad y obtiene resultados.\par
\par \bullet Integra respaldo a Drive al finalizar cada tarea.\par
\par \bullet BiMonthlyScheduler:\par
\par \bullet Determina ventana (1–2, 15–16) y respetar hora configurada.\par
\par \bullet Construye plan (por defecto inmuebles24 venta/renta, expandible) y ejecuta (puede ser remoto via SSH executor).\par
\par
\par\fs24 \b Ejecución remota por SSH (Windows → Ubuntu)\b0\fs22 \par
\par
\par \bullet remote_executor.py (Paramiko):\par
\par \bullet connect(): abre SSH + SFTP, verifica sistema remoto.\par
\par \bullet sync_project_files(): sincroniza código (excluye patrones definidos).\par
\par \bullet deploy_project(): instala requirements y crea estructura de data.\par
\par \bullet execute_scraper(): ejecuta un scraper con parámetros y captura salida.\par
\par
Requisitos previos en el servidor (Ubuntu 24): Python3, pip, Chrome estable, rclone configurado, usuario scraper con venv.\par
\par
\par\fs24 \b Respaldo a Google Drive (rclone)\b0\fs22 \par
\par
\par \bullet gdrive_backup_manager.py:\par
\par \bullet Verifica remoto rclone (gdrive:).\par
\par \bullet Crea estructura de directorios espejo de data/.\par
\par \bullet Copia CSVs en lotes; guarda historial de respaldos.\par
\par \bullet start_automatic_backup(): loop en background que detecta archivos nuevos/modificados y los sube.\par
\par
Recomendado: iniciar el modo automático al comenzar la orquestación para que el respaldo no bloquee scrapers.\par
\par
\par\fs24 \b Monitoreo y logs\b0\fs22 \par
\par
\par \bullet performance_monitor.py: registra CPU/Memoria/Disco/Red, con thresholds y reportes.\par
\par \bullet Logs por módulo con timestamps en logs/.\par
\par \bullet bimonthly_scheduler guarda execution_history.json con resumen de corridas.\par
\par
\par\fs24 \b Configuración clave\b0\fs22 \par
\par
\par \bullet config/dell_t710_config.yaml: límites de recursos, horarios (02:00), rutas remotas y python_env, backup base path.\par
\par \bullet config/ssh_config.json: host/puerto/usuario/llave y paths remotos (sin comentarios).\par
\par \bullet requirements.txt: dependencias (agregados python-docx y markdown-it-py para esta guía).\par
\par
\par\fs24 \b Puesta en marcha (Windows 11)\b0\fs22 \par
\par
1) Instalar dependencias del proyecto en el entorno Python local.\par
2) Desplegar en el servidor (opcional si se orquesta localmente): remote_executor --deploy.\par
3) Probar un scraper corto headless (5 páginas). \par
4) Iniciar orquestación de prueba (concurrent_manager --test) y validar logs.\par
5) Configurar rclone y probar backup (utils/gdrive_backup_manager.py --test-rclone / --backup-now).\par
\par
\par\fs24 \b Operación diaria\b0\fs22 \par
\par
\par \bullet Modo programado: iniciar bimonthly_scheduler en background (o usar un servicio/systemd en el servidor).\par
\par \bullet Monitoreo: consultar performance_monitor --status y revisar logs.\par
\par \bullet Respaldos: mantener rclone configurado; opcional encender modo automático desde la orquestación.\par
\par
\par\fs24 \b Cómo agregar un nuevo sitio o scraper\b0\fs22 \par
\par
1) Crear scraper en scrapers/ (ej. lamudi_scraper.py) con clase LamudiScraper:\par
\par \bullet Entradas: headless: bool, max_pages: int, operation_type: "venta|renta".\par
\par \bullet Método run() → dict: \{success: bool, total_properties: int, output_files: [paths], metadata_path: str\}.\par
\par \bullet Respetar tiempos, waits y técnicas de anti-detección usadas en inmuebles24_professional.\par
\par \bullet Guardar CSV+metadata en la ruta data/[sitio]/[operacion]/[Mes Año]/[ejecución]/.\par
2) Integrar en ConcurrentScraperManager:\par
\par \bullet Importar la clase.\par
\par \bullet Agregar case en run_scraper_process() para construir e invocar el scraper según config['site'].\par
3) Agregar al plan del BiMonthlyScheduler:\par
\par \bullet Extender create_execution_plan() con entradas para el nuevo sitio (venta/renta y páginas estimadas).\par
4) Validar en orquestación:\par
\par \bullet Ejecutar concurrent_manager --test con el nuevo sitio en el plan y verificar éxito y logs.\par
5) Respaldo:\par
\par \bullet gdrive_backup_manager no requiere cambios; sube cualquier CSV encontrado en data/.\par
6) Documentación:\par
\par \bullet Opcional: actualizar docs/ con notas específicas del sitio, selectores clave o límites.\par
\par
Para scrapers de dos fases (ej. principal → *_unico):\par
\par \bullet Ejecutar el scraper principal primero; guardar el CSV base.\par
\par \bullet Implementar el scraper *_unico que tome el CSV base como entrada, recorra los enlaces o IDs y enriquezca registros.\par
\par \bullet Orquestación: modelar como dos tareas secuenciales para ese site; al terminar la primera, encolar la segunda con referencia al archivo generado.\par
\par
\par\fs24 \b Resolución de problemas\b0\fs22 \par
\par
\par \bullet SSH falla: revisar conexión, llave, config/ssh_config.json y reachability (ping). \par
\par \bullet Cloudflare/bloqueos: ajustar timings, headers, y confirmar que se usa el perfil headless correcto.\par
\par \bullet Uso alto de recursos: bajar max_concurrent en concurrent_manager o en la config; revisar performance_monitor.\par
\par \bullet rclone no detecta remoto: ejecutar rclone config y verificar que exista gdrive:.\par
\par \bullet JSON inválidos: recordar que ssh_config.json no admite comentarios; usar campo _comment si se necesita.\par
\par
\par\fs24 \b Referencias rápidas\b0\fs22 \par
\par
\par \bullet Ejecutar un scraper local:\par
\par \bullet python scrapers/inmuebles24_professional.py --headless --operation=venta --pages=100\par
\par \bullet Orquestación de prueba:\par
\par \bullet python orchestrator/concurrent_manager.py --test\par
\par \bullet Scheduler (estado / inicio / forzar ejecución):\par
\par \bullet python orchestrator/bimonthly_scheduler.py --status|--start|--run-now\par
\par \bullet Despliegue SSH / estado remoto / test remoto:\par
\par \bullet python ssh_deployment/remote_executor.py --deploy|--status|--test-scraper inmuebles24 --pages 5\par
\par \bullet Backup a Drive:\par
\par \bullet python utils/gdrive_backup_manager.py --test-rclone|--backup-now|--start-auto|--status\par
\par
---\par
Versión: 1.0 • Fecha: 2025-08-29 • Destino: Dell T710 (Ubuntu 24) • Control: Windows 11\par
}\n