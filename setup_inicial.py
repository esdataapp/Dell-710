#!/usr/bin/env python3
"""
Setup Inicial - PropertyScraper Dell710
Script para configurar el sistema validando los archivos CSV en ``URLs/``
"""

import os
import sys
import csv
import json
import logging
from pathlib import Path
from datetime import datetime

def setup_logging():
    """Configurar logging para setup"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = Path('logs') / f'setup_inicial_{timestamp}.log'
    log_file.parent.mkdir(exist_ok=True, parents=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)8s | SETUP | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

def check_urls_files():
    """Verificar que los CSV en URLs/ existen y tienen la estructura correcta"""
    logger = logging.getLogger(__name__)
    logger.info("üìÑ Verificando archivos CSV en URLs/...")

    urls_dir = Path('URLs')
    if not urls_dir.exists():
        logger.error("‚ùå Directorio URLs/ no encontrado")
        return False

    csv_files = list(urls_dir.glob('*.csv'))
    if not csv_files:
        logger.error("‚ùå No se encontraron archivos CSV en URLs/")
        return False

    required_columns = ['PaginaWeb', 'Ciudad', 'Operacion', 'ProductoPaginaWeb', 'URL']
    all_valid = True

    for csv_file in csv_files:
        try:
            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                rows = list(reader)

            if not rows:
                logger.warning(f"‚ö†Ô∏è {csv_file.name} est√° vac√≠o")
                continue

            available = [c.replace('Operaci√≥n', 'Operacion') for c in rows[0].keys()]
            missing = [col for col in required_columns if col not in available]
            if missing:
                logger.error(f"‚ùå Columnas faltantes en {csv_file.name}: {missing}")
                all_valid = False
                continue

            logger.info(f"‚úÖ {csv_file.name}: {len(rows)} URLs")
        except Exception as e:
            logger.error(f"‚ùå Error leyendo {csv_file.name}: {e}")
            all_valid = False

    return all_valid

def create_directory_structure():
    """Crear estructura de directorios necesaria"""
    logger = logging.getLogger(__name__)
    logger.info("üìÅ Creando estructura de directorios...")
    
    directories = [
        'data',
        'logs',
        'logs/checkpoints',
        'logs/page_samples',
        'config',
        'scrapers',
        'orchestrator',
        'utils',
        'monitoring',
        'ssh_deployment'
    ]
    
    for dir_name in directories:
        dir_path = Path(dir_name)
        dir_path.mkdir(exist_ok=True, parents=True)
        logger.info(f"üìÇ {dir_name}/")
    
    logger.info("‚úÖ Estructura de directorios creada")

def initialize_registry():
    """Inicializar el registry con las URLs del CSV"""
    logger = logging.getLogger(__name__)
    logger.info("üóÇÔ∏è Inicializando registry...")
    
    try:
        # Verificar que el archivo utils/enhanced_scraps_registry.py existe
        registry_file = Path('utils') / 'enhanced_scraps_registry.py'
        if not registry_file.exists():
            logger.error("‚ùå utils/enhanced_scraps_registry.py no encontrado")
            return False

        # Importar y usar el registry desde utils/
        sys.path.append(str(Path('utils').resolve()))
        from utils.enhanced_scraps_registry import EnhancedScrapsRegistry
        
        registry = EnhancedScrapsRegistry()
        
        # Cargar URLs y generar scraps
        logger.info("üì• Cargando URLs desde archivos en URLs/...")
        urls = registry.load_urls_from_csv()
        logger.info(f"‚úÖ {len(urls)} URLs cargadas")
        
        # Obtener estad√≠sticas
        stats = registry.get_registry_stats()
        logger.info("üìä Registry inicializado:")
        logger.info(f"   Scraps activos: {stats.get('scraps_activos', 'N/A')}")
        logger.info(f"   Total scraps: {stats.get('total_scraps', 'N/A')}")
        
        return True
        
    except ImportError as e:
        logger.error(f"‚ùå Error importando registry: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error inicializando registry: {e}")
        return False

def check_dependencies():
    """Verificar dependencias de Python"""
    logger = logging.getLogger(__name__)
    logger.info("üêç Verificando dependencias de Python...")
    
    required_packages = [
        'seleniumbase',
        'pandas',
        'psutil',
        'pathlib'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"‚úÖ {package}")
        except ImportError:
            logger.warning(f"‚ùå {package} no encontrado")
            missing_packages.append(package)
    
    if missing_packages:
        logger.warning("‚ö†Ô∏è Instalar paquetes faltantes:")
        logger.warning(f"pip install {' '.join(missing_packages)}")
        return False
    
    logger.info("‚úÖ Todas las dependencias est√°n instaladas")
    return True

def check_scrapers():
    """Verificar que los scrapers existen"""
    logger = logging.getLogger(__name__)
    logger.info("ü§ñ Verificando scrapers...")
    
    scrapers_dir = Path('scrapers')
    required_scrapers = [
        'inm24.py',
        'cyt.py',
        'mit.py',
        'lam.py',
        'prop.py',
        'tro.py'
    ]
    
    missing_scrapers = []
    
    for scraper in required_scrapers:
        scraper_path = scrapers_dir / scraper
        if scraper_path.exists():
            logger.info(f"‚úÖ {scraper}")
        else:
            logger.warning(f"‚ùå {scraper} no encontrado")
            missing_scrapers.append(scraper)
    
    if missing_scrapers:
        logger.warning("‚ö†Ô∏è Scrapers faltantes encontrados")
        return False
    
    logger.info("‚úÖ Todos los scrapers est√°n disponibles")
    return True

def create_config_files():
    """Crear archivos de configuraci√≥n b√°sicos"""
    logger = logging.getLogger(__name__)
    logger.info("‚öôÔ∏è Creando archivos de configuraci√≥n...")
    
    # Configuraci√≥n b√°sica del orquestador
    orchestrator_config = {
        'max_concurrent_websites': 4,
        'max_cpu_usage': 80,
        'max_memory_usage': 80,
        'checkpoint_interval': 50,
        'default_page_limit': 50,
        'anti_detection': {
            'min_delay': 2,
            'max_delay': 4,
            'user_agents_rotation': True,
            'random_viewport': True
        }
    }
    
    config_dir = Path('config')
    config_dir.mkdir(exist_ok=True, parents=True)
    
    with open(config_dir / 'orchestrator_config.json', 'w', encoding='utf-8') as f:
        json.dump(orchestrator_config, f, indent=2, ensure_ascii=False)
    
    logger.info("üìÑ orchestrator_config.json creado")
    
    # Configuraci√≥n de scrapers
    scraper_config = {
        'headless_mode': True,
        'timeout': 30,
        'max_retries': 3,
        'page_load_strategy': 'normal',
        'anti_detection': {
            'stealth_mode': True,
            'disable_images': False,
            'random_user_agent': True
        }
    }
    
    with open(config_dir / 'scraper_config.json', 'w', encoding='utf-8') as f:
        json.dump(scraper_config, f, indent=2, ensure_ascii=False)
    
    logger.info("üìÑ scraper_config.json creado")
    logger.info("‚úÖ Archivos de configuraci√≥n creados")

def setup_complete_check():
    """Verificaci√≥n final de que todo est√° listo"""
    logger = logging.getLogger(__name__)
    logger.info("üîç Verificaci√≥n final del setup...")
    
    checks = [
        ("Archivos CSV en URLs/", check_urls_files()),
        ("Dependencias Python", check_dependencies()),
        ("Scrapers", check_scrapers()),
        ("Registry", initialize_registry())
    ]
    
    passed_checks = 0
    total_checks = len(checks)
    
    logger.info("\n" + "="*60)
    logger.info("üìã RESUMEN DE VERIFICACIONES")
    logger.info("="*60)
    
    for check_name, result in checks:
        status = "‚úÖ OK" if result else "‚ùå FALLO"
        logger.info(f"{status:8} {check_name}")
        if result:
            passed_checks += 1
    
    logger.info("="*60)
    logger.info(f"üéØ Resultado: {passed_checks}/{total_checks} verificaciones exitosas")
    
    if passed_checks == total_checks:
        logger.info("üéâ ¬°Setup completado exitosamente!")
        logger.info("\nüìù Pr√≥ximos pasos:")
        logger.info("1. Ejecutar test de integraci√≥n: python test_integration.py")
        logger.info("2. Ejecutar orquestador: python orchestrator/advanced_orchestrator.py")
        logger.info("3. Monitorear logs en: logs/")
        return True
    else:
        logger.warning("‚ö†Ô∏è Setup incompleto. Resolver los fallos antes de continuar.")
        return False

def main():
    """Funci√≥n principal del setup"""
    print("üöÄ PropertyScraper Dell710 - Setup Inicial")
    print("="*60)
    
    logger = setup_logging()
    
    logger.info("üîß Iniciando configuraci√≥n inicial del sistema...")
    
    # Crear estructura de directorios
    create_directory_structure()
    
    # Crear archivos de configuraci√≥n
    create_config_files()
    
    # Verificaci√≥n completa
    success = setup_complete_check()
    
    if success:
        logger.info("\n‚úÖ Sistema listo para usar!")
        return 0
    else:
        logger.error("\n‚ùå Setup incompleto")
        return 1

if __name__ == "__main__":
    sys.exit(main())
